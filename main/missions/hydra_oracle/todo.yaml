TODO:
    - simplify policies.py
    - find the update weights step somewhere in policies.py or a2c_exposed.py
    - see if the network can be swapped out with an LSTM, or if an LSTM can be added on top of the latent space created by the MLP

Plan:
    - failed: finding memory across large timesteps, ex; the konami code
    - found: attention matrix is what is needed, but I don't fully understand it
    - found: LSTM applied AFTER an encoding process would likely work well, but are not especially novel
    - innovation options:
        - getting the primary agent to optimally explore
        - better state compression with multi-head options
        - using transformers for memory AFTER an encoding process

Flow of A2C:
    - init's
    - learn:
        - _setup_learn:
        - for each rollout batch
            - collect a rollout
                - policy.set_training_mode(False)
                - policy.reset_noise(env.num_envs)
                - policy(obs_tensor) # observation to action (?)
                - policy.obs_to_tensor(infos[idx]["terminal_observation"])[0]
                - policy.predict_values(terminal_obs)[0]
                - policy.predict_values(obs_as_tensor(new_obs, self.device))
            - run train on a rollout